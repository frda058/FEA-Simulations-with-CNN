{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import skorch\n",
    "from skorch.regressor import NeuralNetRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed\n",
    "seed = 126\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "random_seed = 126 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import excel files and save as parquet\n",
    "**Convert to parquet to improve file read times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_df = pd.read_excel('data/features_all.xlsx', header=None)\n",
    "X_df = X_df1.T\n",
    "X_df = X_df1.iloc[:, 1:]\n",
    "X_df.columns = X_df1.columns.astype(str)\n",
    "X_df.to_parquet('data/features_all.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.read_excel('data/labels_all.xlsx', header=None)\n",
    "y_df = y_df_1.rename(index={0: 'E', 1: 'size', 2: 'p', 3: 'v', 4:'c', 5:'G', 6:'K', 7:'G/K'})\n",
    "y_df = y_df_1.T\n",
    "y_df.columns = y_df_1.columns.astype(str)\n",
    "y_df.to_parquet('data/labels_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_parquet('data/features_all.parquet')\n",
    "y_df = pd.read_parquet('data/labels_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values in the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_null = X_df[np.isnan(X_df).any(axis=1)].index.values\n",
    "y_null = y_df[np.isnan(y_df).any(axis=1)].index.values\n",
    "\n",
    "print(len(X_null), len(y_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = pd.Series(np.union1d(ser1, ser2))\n",
    "  \n",
    "# intersection of the series\n",
    "intersect = pd.Series(np.intersect1d(ser1, ser2))\n",
    "  \n",
    "# uncommon elements in both the series \n",
    "notcommonseries = union[~union.isin(intersect)]\n",
    "  \n",
    "# displaying the result\n",
    "print(notcommonseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with null values in both features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X_df.drop(index=list(notcommonseries))\n",
    "y_df = y_df.drop(index=list(notcommonseries))\n",
    "\n",
    "ser1 = X_df[np.isnan(X_df).any(axis=1)].index.values\n",
    "ser2 = y_df[np.isnan(y_df).any(axis=1)].index.values\n",
    "print(len(ser1), len(ser2))\n",
    "\n",
    "X_df = X_df[~np.isnan(X_df).any(axis=1)]\n",
    "y_df = y_df[~np.isnan(y_df).any(axis=1)]\n",
    "\n",
    "print(X_df.shape, y_df.shape)\n",
    "print(X_df.isnull().values.any())\n",
    "print(y_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take every other signal data point to reduce size of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = list(range(2,X_df.shape[1],2)) #Indexes to drop\n",
    "\n",
    "drop_cols = [j for i,j in enumerate(X_df.columns) if i in drop_idx]\n",
    "\n",
    "X_df2 = X_df.drop(drop_cols, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove initial pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 3e-06\n",
    "time_increment = 2e-09\n",
    "\n",
    "sampling_num = int(total_time/time_increment)\n",
    "\n",
    "X_df_ = X_df2.iloc[:, sampling_num:27000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe with the subset of target variables to predict\n",
    "1) Elastic Modulus  \n",
    "2) Size  \n",
    "3) Acoustic Impedance  \n",
    "4) Speed of Sound  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_sub = y_df[['E', 'size', 'Z', 'c']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()    # Scaler for y\n",
    "scaler2 = MinMaxScaler()   # Scaler for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale x\n",
    "X_df_Scaled = (pd.DataFrame(scaler2.fit_transform(X_df_.T))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale y\n",
    "y_df_Scaled = pd.DataFrame(columns=[y_df_sub.columns])\n",
    "y_df_Scaled = scaler.fit_transform(y_df_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, fc_out):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        # Convolution block\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=5, stride=3, padding=2), #out_channels: number of filters/kernel\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=8, kernel_size=5, stride=3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=3, padding=0)\n",
    "        )\n",
    "\n",
    "        # Fully connected block\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features = 16*197, out_features = 200),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, fc_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = x.view(-1, 16*197)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNN to predict elastic modulus, size, impedance, and speed of sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = 5\n",
    "lr = 0.0001\n",
    "fc_out = 4\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "validation_size_RMSE = []\n",
    "validation_size_MAPE = []\n",
    "validation_size_MAE = []\n",
    "\n",
    "validation_EM_RMSE = []\n",
    "validation_EM_MAPE = []\n",
    "validation_EM_MAE = []\n",
    "\n",
    "validation_c_RMSE = []\n",
    "validation_c_MAPE = []\n",
    "validation_c_MAE = []\n",
    "\n",
    "validation_Z_RMSE = []\n",
    "validation_Z_MAPE = []\n",
    "validation_Z_MAE = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(instances):\n",
    "    \n",
    "    print(\"Split #\", i+1)\n",
    "    \n",
    "    other_x, test_x, other_y, test_y = train_test_split(X_df_Scaled.values, y_df_Scaled, \n",
    "                                                    test_size = 0.10, random_state = seed*i)\n",
    "\n",
    "    x_train_true = torch.unsqueeze(torch.from_numpy(other_x).float(), 1)\n",
    "\n",
    "    x_val_true = torch.unsqueeze(torch.from_numpy(test_x).float(), 1)\n",
    "\n",
    "    y_train_true = torch.from_numpy(other_y).float()\n",
    "\n",
    "    y_val_true = torch.from_numpy(test_y).float()\n",
    "\n",
    "\n",
    "    # CNN model\n",
    "    \n",
    "    model = MyNet(fc_out)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "\n",
    "    # Variable to store epoch and loss for plotting\n",
    "    epoch_list = []\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output_t = model(x_train_true)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss_t = criterion(output_t, y_train_true)\n",
    "\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss_t.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # prep model for evaluation\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_v = model(x_val_true)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss_v = criterion(output_v, y_val_true)\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            epoch_list.append(epoch+1)\n",
    "            loss_train.append(loss_t.item())\n",
    "            loss_val.append(loss_v.item())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Train Loss: {:.10f}'.format(epoch + 1, num_epochs, loss_t))\n",
    "            print('Epoch [{}/{}], Validation Loss: {:.10f}'.format(epoch + 1, num_epochs, loss_v))\n",
    "            print()\n",
    "            #print(loss_v.item())\n",
    "\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epoch_list, loss_train, '-o', label='Training loss')\n",
    "    plt.plot(epoch_list, loss_val, '-o', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.title(f' CNN Model (lr = {lr})', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Loss_{i}.png', dpi=1200, format = 'png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #y_other_true = y_other_true.detach().numpy()\n",
    "    #y_other_pred = y_other_pred.detach().numpy()\n",
    "\n",
    "    y_val_true = y_val_true.detach().numpy()\n",
    "    y_val_pred = output_v.detach().numpy()\n",
    "\n",
    "    unscaled_pred = scaler.inverse_transform(y_val_pred)\n",
    "    unscaled_true = scaler.inverse_transform(y_val_true)\n",
    "    \n",
    "\n",
    "    unscaled_E_pred = unscaled_pred[:, 0]\n",
    "    unscaled_E_true = unscaled_true[:, 0]\n",
    "    \n",
    "    unscaled_size_pred = unscaled_pred[:, 1]\n",
    "    unscaled_size_true = unscaled_true[:, 1]\n",
    "    \n",
    "    unscaled_Z_pred = unscaled_pred[:, 2]\n",
    "    unscaled_Z_true = unscaled_true[:, 2]\n",
    "    \n",
    "    unscaled_c_pred = unscaled_pred[:, 3]\n",
    "    unscaled_c_true = unscaled_true[:, 3]\n",
    "     \n",
    "\n",
    "    # Calculate RMSE for val data\n",
    "    RMSE_EM = np.sqrt(mean_squared_error(unscaled_E_true, unscaled_E_pred))\n",
    "    print('Elastic Modulus RMSE = ', RMSE_EM)\n",
    "\n",
    "    RMSE_Size = np.sqrt(mean_squared_error(unscaled_size_true, unscaled_size_pred))\n",
    "    print('Size RMSE = ', RMSE_Size)\n",
    "    print()\n",
    "    \n",
    "    RMSE_Z = np.sqrt(mean_squared_error(unscaled_Z_true, unscaled_Z_pred))\n",
    "    print('Impedance RMSE = ', RMSE_Z)\n",
    "    print()\n",
    "    \n",
    "    RMSE_c = np.sqrt(mean_squared_error(unscaled_c_true, unscaled_c_pred))\n",
    "    print('SoS RMSE = ', RMSE_c)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Calculate MAPE for val data\n",
    "\n",
    "    MAPE_EM = mean_absolute_percentage_error(unscaled_E_true, unscaled_E_pred)\n",
    "    print('Elastic Modulus MAPE = ', MAPE_EM)\n",
    "\n",
    "    MAPE_Size = mean_absolute_percentage_error(unscaled_size_true, unscaled_size_pred)\n",
    "    print('Size MAPE = ', MAPE_Size)\n",
    "    print()\n",
    "      \n",
    "    MAPE_Z = np.sqrt(mean_absolute_percentage_error(unscaled_Z_true, unscaled_Z_pred))\n",
    "    print('Impedance RMSE = ', MAPE_Z)\n",
    "    print()\n",
    "    \n",
    "    MAPE_c = np.sqrt(mean_absolute_percentage_error(unscaled_c_true, unscaled_c_pred))\n",
    "    print('SoS RMSE = ', MAPE_c)\n",
    "    print()\n",
    "  \n",
    "    \n",
    "\n",
    "    # Calculate MAE for val data\n",
    "    errors_EM = mean_absolute_error(unscaled_E_true, unscaled_E_pred)\n",
    "    print('Elastic Modulus MAE = ', errors_EM)\n",
    "    print()\n",
    "\n",
    "    errors_Size = mean_absolute_error(unscaled_size_true, unscaled_size_pred)\n",
    "    print('Size MAE = ', errors_Size)\n",
    "    print()\n",
    "    \n",
    "    errors_Z = np.sqrt(mean_absolute_error(unscaled_Z_true, unscaled_Z_pred))\n",
    "    print('Impedance RMSE = ', errors_Z)\n",
    "    print()\n",
    "        \n",
    "    errors_c = np.sqrt(mean_absolute_error(unscaled_c_true, unscaled_c_pred))\n",
    "    print('SoS RMSE = ', errors_c)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    validation_size_RMSE.append(RMSE_Size)\n",
    "    validation_size_MAPE.append(MAPE_Size)\n",
    "    validation_size_MAE.append(errors_Size)\n",
    "\n",
    "    validation_EM_RMSE.append(RMSE_EM)\n",
    "    validation_EM_MAPE.append(MAPE_EM)\n",
    "    validation_EM_MAE.append(errors_EM)\n",
    "    \n",
    "    validation_c_RMSE.append(RMSE_c)\n",
    "    validation_c_MAPE.append(MAPE_c)\n",
    "    validation_c_MAE.append(errors_c)\n",
    "    \n",
    "    validation_Z_RMSE.append(RMSE_Z)\n",
    "    validation_Z_MAPE.append(MAPE_Z)\n",
    "    validation_Z_MAE.append(errors_Z)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(unscaled_E_true, unscaled_E_pred, c = 'darkviolet', s=100, alpha=0.5, marker = \"o\")\n",
    "    p1 = max(max(unscaled_E_pred), max(unscaled_E_true))\n",
    "    p2 = min(min(unscaled_E_pred), min(unscaled_E_true))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Actual Values (kPa)', fontsize=15)\n",
    "    plt.ylabel('Predictions (kPa)', fontsize=15)\n",
    "    plt.suptitle('Elastic Modulus', fontweight = 'bold', fontsize=20)\n",
    "    plt.title(f'MAPE = {MAPE_EM:.3f}, MAE = {errors_EM:.3f}, RMSE = {RMSE_EM:.3f}')\n",
    "    #plt.xticks(rotation=45)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'EM_{i}.png', dpi=1200, format = 'png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(unscaled_size_true*1000, unscaled_size_pred*1000, c = 'crimson', s=100, alpha=0.5, marker = \"o\")\n",
    "    p1 = max(max(unscaled_size_pred), max(unscaled_size_true))*1000\n",
    "    p2 = min(min(unscaled_size_pred), min(unscaled_size_true))*1000\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Actual Values (mm)', fontsize=15)\n",
    "    plt.ylabel('Predictions (mm)', fontsize=15)\n",
    "    plt.suptitle('Size', fontweight = 'bold', fontsize=20)\n",
    "    plt.axis('equal')\n",
    "    plt.title(f'MAPE = {MAPE_Size:.3f}, MAE = {errors_Size*1000:.3f}, RMSE = {RMSE_Size*1000:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Size_{i}.png', dpi=1200, format = 'png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(unscaled_c_true, unscaled_c_pred, c = 'green', s=100, alpha=0.5, marker = \"o\")\n",
    "    p1 = max(max(unscaled_c_pred), max(unscaled_c_true))\n",
    "    p2 = min(min(unscaled_c_pred), min(unscaled_c_true))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Actual Values (m/s)', fontsize=15)\n",
    "    plt.ylabel('Predictions (m/s)', fontsize=15)\n",
    "    plt.suptitle('Speed of Sound', fontweight = 'bold', fontsize=20)\n",
    "    plt.axis('equal')\n",
    "    plt.title(f'MAPE = {MAPE_c:.3f}, MAE = {errors_c:.3f}, RMSE = {RMSE_c:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'SoS_{i}.png', dpi=1200, format = 'png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(unscaled_Z_true/1000000, unscaled_Z_pred/1000000, c ='orange', s=100, alpha=0.5, marker = \"o\")\n",
    "    p1 = max(max(unscaled_Z_pred), max(unscaled_Z_true))/1000000\n",
    "    p2 = min(min(unscaled_Z_pred), min(unscaled_Z_true))/1000000\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Actual Values (MRayl)', fontsize=15)\n",
    "    plt.ylabel('Predictions (MRayl)', fontsize=15)\n",
    "    plt.suptitle('Acoustic Impedance', fontweight = 'bold', fontsize=20)\n",
    "    plt.axis('equal')\n",
    "    plt.title(f'MAPE = {MAPE_Z:.3f}, MAE = {errors_Z/1000000:.3f}, RMSE = {RMSE_Z/1000000:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Z_{i}.png', dpi=1200, format = 'png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(validation_EM_MAPE), '+/-', np.std(validation_EM_MAPE))\n",
    "print(np.mean(validation_EM_MAE), '+/-', np.std(validation_EM_MAE))\n",
    "print(np.mean(validation_EM_RMSE), '+/-', np.std(validation_EM_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(validation_size_MAPE), '+/-', np.std(validation_size_MAPE))\n",
    "print(np.mean(validation_size_MAE), '+/-', np.std(validation_size_MAE))\n",
    "print(np.mean(validation_size_RMSE),  '+/-', np.std(validation_size_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(validation_c_MAPE),  '+/-', np.std(validation_c_MAPE))\n",
    "print(np.mean(validation_c_MAE),  '+/-', np.std(validation_c_MAE))\n",
    "print(np.mean(validation_c_RMSE),  '+/-', np.std(validation_c_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(validation_Z_MAPE),  '+/-', np.std(validation_Z_MAPE))\n",
    "print(np.mean(validation_Z_MAE)/1000000,  '+/-', np.std(validation_Z_MAE)/1000000)\n",
    "print(np.mean(validation_Z_RMSE)/1000000,  '+/-', np.std(validation_Z_RMSE)/1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
